#!/bin/bash
# =============================================================================
# Azure HistData Push Script
# Pushes HistData backfill CSV to Azure VM and imports into PostgreSQL
# =============================================================================

set -euo pipefail

# Configuration
AZURE_VM_IP="${AZURE_VM_IP:-}"
SSH_KEY="${SSH_KEY:-$HOME/.ssh/azure_backtest_rsa}"
CSV_FILE="${1:-}"
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Colors
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

# Usage
if [[ -z "${CSV_FILE}" ]]; then
    echo -e "${RED}Usage: $0 <csv_file>${NC}"
    echo ""
    echo "Example:"
    echo "  $0 /tmp/histdata_2025.csv"
    echo ""
    echo "This script pushes HistData backfill data to Azure PostgreSQL."
    echo "The CSV should be generated by histdata_download.py"
    exit 1
fi

# Check file exists
if [[ ! -f "${CSV_FILE}" ]]; then
    echo -e "${RED}Error: File not found: ${CSV_FILE}${NC}"
    exit 1
fi

# Get VM IP if not set
if [[ -z "${AZURE_VM_IP}" ]]; then
    echo -e "${BLUE}Getting Azure VM IP...${NC}"
    AZURE_VM_IP=$(az vm show -d -g Backtesting -n backtest-vm --query publicIps -o tsv 2>/dev/null || echo "")
    if [[ -z "${AZURE_VM_IP}" ]]; then
        echo -e "${RED}Error: Could not get VM IP. Is the VM running?${NC}"
        echo "Run: ./scripts/azure_backtest.sh start-vm"
        exit 1
    fi
fi

# Check SSH key
if [[ ! -f "${SSH_KEY}" ]]; then
    echo -e "${RED}Error: SSH key not found: ${SSH_KEY}${NC}"
    exit 1
fi

echo -e "${BLUE}=== Pushing HistData Backfill to Azure ===${NC}"
echo "File: ${CSV_FILE}"
echo "VM IP: ${AZURE_VM_IP}"
echo ""

# Get file stats
FILE_SIZE=$(du -h "${CSV_FILE}" | cut -f1)
LINE_COUNT=$(wc -l < "${CSV_FILE}")
echo "File size: ${FILE_SIZE}"
echo "Total lines: ${LINE_COUNT}"
echo ""

# =============================================================================
# 1. Compress CSV
# =============================================================================
echo -e "${BLUE}[1/4] Compressing CSV...${NC}"
TEMP_DIR=$(mktemp -d)
COMPRESSED_FILE="${TEMP_DIR}/histdata_backfill.csv.gz"

gzip -c "${CSV_FILE}" > "${COMPRESSED_FILE}"
COMPRESSED_SIZE=$(du -h "${COMPRESSED_FILE}" | cut -f1)
echo "  Compressed size: ${COMPRESSED_SIZE}"

# =============================================================================
# 2. Upload to Azure VM
# =============================================================================
echo -e "${BLUE}[2/4] Uploading to Azure VM...${NC}"

# Ensure sync directory exists
ssh -i "${SSH_KEY}" -o StrictHostKeyChecking=accept-new "azureuser@${AZURE_VM_IP}" \
    "mkdir -p /data/sync"

# Upload
scp -i "${SSH_KEY}" "${COMPRESSED_FILE}" "azureuser@${AZURE_VM_IP}:/data/sync/"
echo -e "${GREEN}  Uploaded to /data/sync/histdata_backfill.csv.gz${NC}"

# =============================================================================
# 3. Import on Azure VM
# =============================================================================
echo -e "${BLUE}[3/4] Importing data on Azure VM...${NC}"

ssh -i "${SSH_KEY}" "azureuser@${AZURE_VM_IP}" << 'REMOTE_SCRIPT'
cd /data/sync

echo "  Decompressing..."
gunzip -f histdata_backfill.csv.gz

echo "  Checking file..."
head -3 histdata_backfill.csv
echo "  ..."
tail -3 histdata_backfill.csv

LINE_COUNT=$(wc -l < histdata_backfill.csv)
echo "  Total lines: ${LINE_COUNT}"

echo ""
echo "  Importing into ig_candles (APPEND mode - preserving existing data)..."

# Create temp table for import
docker exec postgres psql -U postgres -d forex -c "
DROP TABLE IF EXISTS histdata_import_temp;
CREATE TABLE histdata_import_temp (
    start_time TIMESTAMP NOT NULL,
    epic VARCHAR NOT NULL,
    timeframe INTEGER NOT NULL,
    open DOUBLE PRECISION NOT NULL,
    high DOUBLE PRECISION NOT NULL,
    low DOUBLE PRECISION NOT NULL,
    close DOUBLE PRECISION NOT NULL,
    volume INTEGER NOT NULL,
    ltv INTEGER,
    data_source VARCHAR(50)
);
"

# Import CSV to temp table
docker exec -i postgres psql -U postgres -d forex -c "\COPY histdata_import_temp FROM '/sync/histdata_backfill.csv' WITH CSV HEADER"

# Get import count
IMPORT_COUNT=$(docker exec postgres psql -U postgres -d forex -t -c "SELECT COUNT(*) FROM histdata_import_temp;")
echo "  Imported to temp table: ${IMPORT_COUNT} rows"

# Add data_source if missing in ig_candles
docker exec postgres psql -U postgres -d forex -c "
ALTER TABLE ig_candles ADD COLUMN IF NOT EXISTS data_source VARCHAR(50);
" 2>/dev/null || true

# Insert from temp table, avoiding duplicates
echo "  Inserting into ig_candles (skipping duplicates)..."
docker exec postgres psql -U postgres -d forex -c "
INSERT INTO ig_candles (start_time, epic, timeframe, open, high, low, close, volume, ltv, data_source)
SELECT start_time, epic, timeframe, open, high, low, close, volume, ltv, data_source
FROM histdata_import_temp
ON CONFLICT (start_time, epic, timeframe) DO NOTHING;
"

# Get final count
FINAL_COUNT=$(docker exec postgres psql -U postgres -d forex -t -c "
SELECT COUNT(*) FROM ig_candles WHERE data_source = 'histdata_backfill';
")
echo "  Total histdata_backfill rows in ig_candles: ${FINAL_COUNT}"

# Clean up temp table
docker exec postgres psql -U postgres -d forex -c "DROP TABLE IF EXISTS histdata_import_temp;"

# Show summary by pair
echo ""
echo "  === Import Summary ==="
docker exec postgres psql -U postgres -d forex -c "
SELECT
    epic,
    data_source,
    MIN(start_time) as earliest,
    MAX(start_time) as latest,
    COUNT(*) as count
FROM ig_candles
WHERE timeframe = 1
GROUP BY epic, data_source
ORDER BY epic, data_source;
"

# Cleanup
rm -f histdata_backfill.csv
REMOTE_SCRIPT

# =============================================================================
# 4. Synthesize higher timeframes
# =============================================================================
echo -e "${BLUE}[4/4] Synthesizing higher timeframes (5m, 15m, 1h, 4h)...${NC}"

ssh -i "${SSH_KEY}" "azureuser@${AZURE_VM_IP}" << 'REMOTE_SCRIPT'
echo "  Creating ig_candles_backtest table if not exists..."
docker exec postgres psql -U postgres -d forex -c "
CREATE TABLE IF NOT EXISTS ig_candles_backtest (
    start_time TIMESTAMP NOT NULL,
    epic VARCHAR NOT NULL,
    timeframe INTEGER NOT NULL,
    open DOUBLE PRECISION NOT NULL,
    high DOUBLE PRECISION NOT NULL,
    low DOUBLE PRECISION NOT NULL,
    close DOUBLE PRECISION NOT NULL,
    volume INTEGER NOT NULL,
    ltv INTEGER,
    resampled_from INTEGER DEFAULT 1,
    created_at TIMESTAMP DEFAULT NOW(),
    PRIMARY KEY (start_time, epic, timeframe)
);
CREATE INDEX IF NOT EXISTS idx_backtest_candles_epic ON ig_candles_backtest(epic);
CREATE INDEX IF NOT EXISTS idx_backtest_candles_epic_tf_time ON ig_candles_backtest(epic, timeframe, start_time DESC);
"

echo "  Generating 5-minute candles..."
docker exec postgres psql -U postgres -d forex -c "
INSERT INTO ig_candles_backtest (start_time, epic, timeframe, open, high, low, close, volume, ltv, resampled_from)
SELECT
    date_trunc('hour', start_time) + INTERVAL '5 min' * FLOOR(EXTRACT(minute FROM start_time) / 5) as start_time,
    epic,
    5 as timeframe,
    (array_agg(open ORDER BY start_time))[1] as open,
    MAX(high) as high,
    MIN(low) as low,
    (array_agg(close ORDER BY start_time DESC))[1] as close,
    SUM(volume) as volume,
    SUM(COALESCE(ltv, 0)) as ltv,
    1 as resampled_from
FROM ig_candles
WHERE timeframe = 1 AND data_source = 'histdata_backfill'
GROUP BY epic, date_trunc('hour', start_time) + INTERVAL '5 min' * FLOOR(EXTRACT(minute FROM start_time) / 5)
ON CONFLICT (start_time, epic, timeframe) DO NOTHING;
"

echo "  Generating 15-minute candles..."
docker exec postgres psql -U postgres -d forex -c "
INSERT INTO ig_candles_backtest (start_time, epic, timeframe, open, high, low, close, volume, ltv, resampled_from)
SELECT
    date_trunc('hour', start_time) + INTERVAL '15 min' * FLOOR(EXTRACT(minute FROM start_time) / 15) as start_time,
    epic,
    15 as timeframe,
    (array_agg(open ORDER BY start_time))[1] as open,
    MAX(high) as high,
    MIN(low) as low,
    (array_agg(close ORDER BY start_time DESC))[1] as close,
    SUM(volume) as volume,
    SUM(COALESCE(ltv, 0)) as ltv,
    1 as resampled_from
FROM ig_candles
WHERE timeframe = 1 AND data_source = 'histdata_backfill'
GROUP BY epic, date_trunc('hour', start_time) + INTERVAL '15 min' * FLOOR(EXTRACT(minute FROM start_time) / 15)
ON CONFLICT (start_time, epic, timeframe) DO NOTHING;
"

echo "  Generating 1-hour candles..."
docker exec postgres psql -U postgres -d forex -c "
INSERT INTO ig_candles_backtest (start_time, epic, timeframe, open, high, low, close, volume, ltv, resampled_from)
SELECT
    date_trunc('hour', start_time) as start_time,
    epic,
    60 as timeframe,
    (array_agg(open ORDER BY start_time))[1] as open,
    MAX(high) as high,
    MIN(low) as low,
    (array_agg(close ORDER BY start_time DESC))[1] as close,
    SUM(volume) as volume,
    SUM(COALESCE(ltv, 0)) as ltv,
    1 as resampled_from
FROM ig_candles
WHERE timeframe = 1 AND data_source = 'histdata_backfill'
GROUP BY epic, date_trunc('hour', start_time)
ON CONFLICT (start_time, epic, timeframe) DO NOTHING;
"

echo "  Generating 4-hour candles..."
docker exec postgres psql -U postgres -d forex -c "
INSERT INTO ig_candles_backtest (start_time, epic, timeframe, open, high, low, close, volume, ltv, resampled_from)
SELECT
    date_trunc('day', start_time) + INTERVAL '4 hour' * FLOOR(EXTRACT(hour FROM start_time) / 4) as start_time,
    epic,
    240 as timeframe,
    (array_agg(open ORDER BY start_time))[1] as open,
    MAX(high) as high,
    MIN(low) as low,
    (array_agg(close ORDER BY start_time DESC))[1] as close,
    SUM(volume) as volume,
    SUM(COALESCE(ltv, 0)) as ltv,
    1 as resampled_from
FROM ig_candles
WHERE timeframe = 1 AND data_source = 'histdata_backfill'
GROUP BY epic, date_trunc('day', start_time) + INTERVAL '4 hour' * FLOOR(EXTRACT(hour FROM start_time) / 4)
ON CONFLICT (start_time, epic, timeframe) DO NOTHING;
"

echo ""
echo "  === Timeframe Summary ==="
docker exec postgres psql -U postgres -d forex -c "
SELECT
    epic,
    timeframe,
    MIN(start_time) as earliest,
    MAX(start_time) as latest,
    COUNT(*) as candles
FROM ig_candles_backtest
WHERE start_time >= '2025-01-01' AND start_time < '2025-09-18'
GROUP BY epic, timeframe
ORDER BY epic, timeframe;
"
REMOTE_SCRIPT

# Cleanup temp files
rm -rf "${TEMP_DIR}"

echo ""
echo -e "${GREEN}=== HistData Push Complete ===${NC}"
echo ""
echo "Verification commands (run on Azure):"
echo "  ./scripts/azure_backtest.sh ssh"
echo ""
echo "  # Check 1m data"
echo "  docker exec postgres psql -U postgres -d forex -c \"SELECT epic, data_source, COUNT(*) FROM ig_candles WHERE timeframe=1 GROUP BY epic, data_source ORDER BY epic;\""
echo ""
echo "  # Run test backtest on Feb 2025 data"
echo "  docker exec task-worker python /app/forex_scanner/bt.py EURUSD 30 --start-date 2025-02-01"
