import asyncio
import time
from igstream.chart_streamer import stream_chart_candles
from igstream.ig_auth_prod import ig_login
from igstream.gap_detector import GapDetector
from services.keyvault import get_secret
from services.db import SessionLocal
from services.models import IGCandle
from config import ACTIVE_EPICS
from sqlalchemy import desc
import logging
from datetime import datetime, timezone, timedelta

logger = logging.getLogger(__name__)

# Constants
STREAM_REFRESH_INTERVAL = 60         # How often to check stream status
AUTH_REFRESH_INTERVAL = 55 * 60      # IG tokens expire hourly
RECONNECT_BACKOFF = 60               # Minimum wait before reconnecting
# Staleness thresholds per timeframe (in seconds)
STALENESS_THRESHOLDS = {
    5: 300,      # 5m candles: stale after 5 minutes
    15: 900,     # 15m candles: stale after 15 minutes  
    60: 3900,    # 60m candles: stale after 65 minutes (allows for slight delays)
}

# ‚úÖ Define the epics you want to stream (imported from config)
EPICS = ACTIVE_EPICS

stream_tasks = {}
last_reconnect_attempt = {}
stream_status = {}
headers = {}
gap_detector = GapDetector(max_gap_hours=6)  # Initialize gap detector

def market_is_open() -> bool:
    """Check if forex market is open"""
    now = datetime.now(timezone.utc)
    # IG closes Friday 21:00 UTC and reopens Sunday 21:00 UTC
    if now.weekday() == 5:  # Saturday
        return False
    if now.weekday() == 6 and now.hour < 21:  # Sunday before 21:00 UTC
        return False
    if now.weekday() == 4 and now.hour >= 21:  # Friday after 21:00 UTC
        return False
    return True

async def check_data_freshness(epic: str, timeframe: int = 5) -> bool:
    """
    Check if we have received fresh data for an epic within the threshold
    Returns True if data is fresh, False if stale
    """
    try:
        with SessionLocal() as session:
            # Get the most recent candle for this epic and timeframe
            latest_candle = session.query(IGCandle).filter(
                IGCandle.epic == epic,
                IGCandle.timeframe == timeframe
            ).order_by(desc(IGCandle.start_time)).first()
            
            if not latest_candle:
                logger.warning(f"No candle data found for {epic} {timeframe}m")
                return False
            
            # Check how old the latest data is
            now = datetime.now(timezone.utc)
            
            # Ensure latest_candle timestamp is timezone-aware
            latest_time = latest_candle.start_time
            if latest_time.tzinfo is None:
                latest_time = latest_time.replace(tzinfo=timezone.utc)
            
            time_since_last_candle = (now - latest_time).total_seconds()
            
            # Get appropriate threshold for this timeframe
            staleness_threshold = STALENESS_THRESHOLDS.get(timeframe, 300)  # Default to 5 minutes
            is_fresh = time_since_last_candle < staleness_threshold
            
            if not is_fresh:
                logger.warning(f"Data for {epic} {timeframe}m is stale. Last update: {latest_time} "
                             f"({time_since_last_candle:.0f} seconds ago, threshold: {staleness_threshold}s)")
            else:
                logger.debug(f"Data for {epic} {timeframe}m is fresh. Last update: {latest_time} "
                           f"({time_since_last_candle:.0f} seconds ago)")
            
            return is_fresh
            
    except Exception as e:
        logger.error(f"Error checking data freshness for {epic}: {e}")
        return False

async def check_all_epics_data_freshness() -> dict:
    """Check data freshness for all epics"""
    freshness_status = {}
    
    for epic in EPICS:
        # Check both 5-minute and hourly data
        fresh_5m = await check_data_freshness(epic, 5)
        fresh_60m = await check_data_freshness(epic, 60)
        
        freshness_status[epic] = {
            "5m": fresh_5m,
            "60m": fresh_60m,
            # Consider overall healthy if 5m is fresh (most critical) OR if both are stale due to market being closed
            "overall": fresh_5m  # 5m is the primary indicator of active streaming
        }
    
    return freshness_status

async def get_database_stats():
    """Get some basic stats about recent database activity"""
    try:
        with SessionLocal() as session:
            # Count recent candles (last 30 minutes)
            thirty_minutes_ago = datetime.now(timezone.utc) - timedelta(minutes=30)
            
            recent_count = session.query(IGCandle).filter(
                IGCandle.start_time >= thirty_minutes_ago
            ).count()
            
            # Get latest timestamp
            latest_candle = session.query(IGCandle).order_by(desc(IGCandle.start_time)).first()
            latest_time = latest_candle.start_time if latest_candle else None
            
            return {
                "recent_candles_30m": recent_count,
                "latest_candle_time": latest_time
            }
            
    except Exception as e:
        logger.error(f"Error getting database stats: {e}")
        return {"recent_candles_30m": 0, "latest_candle_time": None}

async def watchdog():
    """Enhanced watchdog that checks streams, data freshness, and gaps"""
    global last_reconnect_attempt
    
    logger.info("‚è±Ô∏è Enhanced watchdog loop with gap detection started...")
    
    # Track gap detection runs
    last_gap_check = 0
    gap_check_interval = 300  # Check for gaps every 5 minutes
    
    while True:
        if not market_is_open():
            logger.info("Market is closed. Skipping stream and data checks.")
            await asyncio.sleep(STREAM_REFRESH_INTERVAL)
            continue

        now = time.time()
        
        # Get database stats
        db_stats = await get_database_stats()
        logger.info(f"üìä Database stats: {db_stats['recent_candles_30m']} candles in last 30min, "
                   f"latest: {db_stats['latest_candle_time']}")
        
        # Check data freshness for all epics
        freshness_status = await check_all_epics_data_freshness()
        
        # Periodically check for gaps and log them
        if now - last_gap_check > gap_check_interval:
            try:
                logger.info("üîç Running gap detection check...")
                gap_stats = gap_detector.get_gap_statistics(EPICS)
                
                if gap_stats["total_gaps"] > 0:
                    logger.warning(f"‚ö†Ô∏è Gap Detection: Found {gap_stats['total_gaps']} gaps "
                                 f"({gap_stats['total_missing_candles']} missing candles)")
                    logger.warning(f"   Recent gaps: {gap_stats['recent_gaps']}, "
                                 f"Largest gap: {gap_stats['largest_gap_minutes']} minutes")
                    
                    # Log gaps by epic
                    for epic, epic_gaps in gap_stats["gaps_by_epic"].items():
                        if epic_gaps["missing_candles"] > 0:
                            logger.warning(f"   {epic}: {epic_gaps['5m']} gaps in 5m, "
                                         f"{epic_gaps['60m']} gaps in 60m "
                                         f"({epic_gaps['missing_candles']} candles)")
                else:
                    logger.info("‚úÖ No gaps detected in candle data")
                
                last_gap_check = now
                
            except Exception as e:
                logger.error(f"Error in gap detection check: {e}")
        
        for epic in EPICS:
            # Check stream task status
            status_info = stream_status.get(epic, {})
            task_status = status_info.get("last_status", "UNKNOWN")
            is_running = status_info.get("running", False)
            
            # Check data freshness
            data_fresh = freshness_status.get(epic, {}).get("overall", False)
            
            last_attempt = last_reconnect_attempt.get(epic, 0)
            time_since_last_attempt = now - last_attempt

            # Determine if reconnect is needed
            stream_issues = not is_running or task_status in ["DISCONNECTED", "STALLED", "UNKNOWN"]
            data_issues = not data_fresh
            
            reconnect_needed = (stream_issues or data_issues) and time_since_last_attempt >= RECONNECT_BACKOFF
            
            if reconnect_needed:
                reason = []
                if stream_issues:
                    reason.append(f"stream_status={task_status}")
                if data_issues:
                    reason.append("stale_data")
                
                reason_str = ", ".join(reason)
                logger.warning(f"‚ö†Ô∏è Stream {epic} needs restart ({reason_str}). Reconnecting...")
                last_reconnect_attempt[epic] = now

                try:
                    await stop_stream(epic)
                    await asyncio.sleep(2)
                    await start_stream(epic)
                    logger.info(f"‚úÖ Stream {epic} restart triggered due to: {reason_str}")
                except Exception as e:
                    logger.exception(f"‚ùå Failed to restart stream {epic}: {e}")
            else:
                # Log status for healthy streams
                if is_running and data_fresh:
                    logger.debug(f"‚úÖ Stream {epic} is healthy (running={is_running}, data_fresh={data_fresh})")

        await asyncio.sleep(STREAM_REFRESH_INTERVAL)

async def start_stream(epic: str):
    """Start streaming for a specific epic"""
    if epic in stream_tasks and not stream_tasks[epic].done():
        logger.info(f"Stream for {epic} is already running. Skipping.")
        return

    global headers
    logger.info(f"üöÄ Starting chart stream for {epic}")
    coroutine = stream_chart_candles(epic, headers)
    task = asyncio.create_task(coroutine)
    stream_tasks[epic] = task
    last_reconnect_attempt[epic] = time.time()
    stream_status[epic] = {"running": True, "last_status": "CONNECTED"}

    def handle_task_result(t):
        try:
            t.result()
        except asyncio.CancelledError:
            logger.info(f"Stream for {epic} cancelled.")
            stream_status[epic] = {"running": False, "last_status": "CANCELLED"}
        except Exception as e:
            logger.error(f"Stream for {epic} crashed: {e}")
            stream_status[epic] = {"running": False, "last_status": "CRASHED"}

    task.add_done_callback(handle_task_result)

async def stop_stream(epic: str):
    """Stop streaming for a specific epic"""
    task = stream_tasks.get(epic)
    if task and not task.done():
        task.cancel()
        try:
            await task
        except asyncio.CancelledError:
            pass
        logger.info(f"Stopped stream for {epic}")
    stream_tasks.pop(epic, None)
    stream_status[epic] = {"running": False, "last_status": "STOPPED"}

async def refresh_auth():
    """Refresh IG authentication tokens"""
    global headers
    logger.info("üîë Refreshing IG session...")
    try:
        api_key = get_secret("prodapikey")
        ig_pwd = get_secret("prodpwd")
        ig_usr = "rehnmarkh"

        auth = await ig_login(api_key, ig_pwd, ig_usr)
        headers = {
            "CST": auth["CST"],
            "X-SECURITY-TOKEN": auth["X-SECURITY-TOKEN"],
            "accountId": auth["ACCOUNT_ID"]
        }
        logger.info("‚úÖ Auth refreshed successfully")
    except Exception as e:
        logger.error(f"‚ùå Failed to refresh auth: {e}")
        raise

async def auth_refresher():
    """Background task to refresh authentication periodically"""
    while True:
        try:
            await refresh_auth()
        except Exception as e:
            logger.error(f"Auth refresh failed: {e}")
        await asyncio.sleep(AUTH_REFRESH_INTERVAL)

async def start_all_streams():
    """Initialize and start all streams"""
    logger.info("üöÄ Starting all forex streams...")
    
    # Initial auth
    await refresh_auth()

    # Start all streams
    for epic in EPICS:
        await start_stream(epic)
        await asyncio.sleep(1)  # Small delay between starts

    logger.info("‚úÖ All streams started.")

    # Start background tasks
    asyncio.create_task(watchdog())
    asyncio.create_task(auth_refresher())
    logger.info("üîÑ Watchdog and auth refresher started.")

async def stop_all_streams():
    """Stop all streams gracefully"""
    logger.info("üõë Stopping all streams...")
    
    # Cancel all stream tasks
    for epic, task in stream_tasks.items():
        if task and not task.done():
            task.cancel()
            try:
                await task
            except asyncio.CancelledError:
                logger.info(f"Stream for {epic} cancelled.")
    
    await asyncio.sleep(0.2)
    logger.info("‚úÖ All streams shut down.")

# Additional utility functions for monitoring
async def get_stream_health_report():
    """Get a comprehensive health report of all streams"""
    if not market_is_open():
        return {"status": "market_closed", "details": "Forex market is currently closed"}
    
    report = {
        "market_open": True,
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "streams": {},
        "database": await get_database_stats()
    }
    
    freshness_status = await check_all_epics_data_freshness()
    
    for epic in EPICS:
        task = stream_tasks.get(epic)
        status_info = stream_status.get(epic, {})
        
        report["streams"][epic] = {
            "task_running": task is not None and not task.done() if task else False,
            "status": status_info.get("last_status", "UNKNOWN"),
            "data_fresh_5m": freshness_status.get(epic, {}).get("5m", False),
            "data_fresh_60m": freshness_status.get(epic, {}).get("60m", False),
            "overall_healthy": (
                task is not None and not task.done() and 
                freshness_status.get(epic, {}).get("overall", False)
            ) if task else False
        }
    
    return report